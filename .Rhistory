png(file = "histograms.png", width = 1200, height = 800)
qplot(data = gg, x = value, facets = ~Var2, bins = 30) + theme_classic()
dev.off()
# 5. Reduction of dimensionality of numeric variables only select those vars, that
# have a reasonable correlation
cormat = cor(df.outlier.trunc[, !names(df.outlier.trunc) %in% c("logSalePrice")])
list   = which(abs(cormat) > 0.7 & abs(cormat) < 1, arr.ind = TRUE)
df.pca = df.outlier.trunc[unique(rownames(list))]
cor(df.pca)
# principal component analysis
prin1 = princomp(df.pca, cor = T, scores = T)
png(file = "screeplot.png", width = 800, height = 800)
screeplot(prin1, type = "l")
dev.off()
summary(prin1)
# we use 4 components, to explain > 70% of variance
scores = prin1$scores[, 1:4]
#to be able to select factor levels automatically, convert all factors into dummies
mm = model.matrix(~. - 1, data = df.merged[, names(colclasses[colclasses == "factor"])])
# make dataframe from remaining numeric vars and PCA-Results and dummy vars
df.preprocessed = cbind(df.outlier.trunc[, !names(df.outlier.trunc) %in% rownames(list)], scores, mm)
#avoid spaces in names
df.preprocessed        = as.data.frame(df.preprocessed)
names(df.preprocessed) = make.names(names(df.preprocessed), unique = TRUE)
#standardize numerics
df.preprocessed[!names(df.preprocessed) %in% colnames(mm)]= scale(df.preprocessed[!names(df.preprocessed) %in% colnames(mm)])
summary(df.preprocessed)
#split into test and trainingsdata
df.preprocessed$Id = 1:nrow(df.preprocessed)
#make 80/20 split
set.seed(123)
ids = sample(df.preprocessed$Id, nrow(df.preprocessed)*0.2)
write.csv(df.preprocessed[df.preprocessed$Id %in% ids,], file = "test_preprocessed.csv")
write.csv(df.preprocessed[!df.preprocessed$Id %in% ids,], file = "train_preprocessed.csv")
devtools::install_github("lborke/yamldebugger")
library(devtools)
devtools::install_github("lborke/yamldebugger")
# load the package every time you want to use 'yamldebugger'
library(yamldebugger)
library(devtools)
install.packages("devtools")
library(devtools)
devtools::install_github("lborke/yamldebugger")
# load the package every time you want to use 'yamldebugger'
library(yamldebugger)
library(devtools)
devtools::install_github("lborke/yamldebugger")
# load the package every time you want to use 'yamldebugger'
library(yamldebugger)
library(devtools)
devtools::install_github("lborke/yamldebugger", force = TRUE)
# load the package every time you want to use 'yamldebugger'
library(yamldebugger)
setwd("C:/Users/Tammena/Documents/SPL-HousePriceRegression/quantlets/Regression_Models")
d_init = yaml.debugger.init(workdir, show_keywords = TRUE)
qnames = yaml.debugger.get.qnames(d_init$RootPath)
d_results = yaml.debugger.run(qnames, d_init)
OverView = yaml.debugger.summary(qnames, d_results, summaryType = "mini")
workdir = "C:/Users/Tammena/Documents/SPL-HousePriceRegression/quantlets/Regression_Models"
d_init = yaml.debugger.init(workdir, show_keywords = TRUE)
qnames = yaml.debugger.get.qnames(d_init$RootPath)
d_results = yaml.debugger.run(qnames, d_init)
OverView = yaml.debugger.summary(qnames, d_results, summaryType = "mini")
workdir = "C:/Users/Tammena/Documents/SPL-HousePriceRegression/quantlets/"
d_init = yaml.debugger.init(workdir, show_keywords = TRUE)
qnames = yaml.debugger.get.qnames(d_init$RootPath)
d_results = yaml.debugger.run(qnames, d_init)
OverView = yaml.debugger.summary(qnames, d_results, summaryType = "mini")
#clear variables and close windows
rm(list = ls(all = TRUE))
graphics.off()
#install and load packages
libraries = c("xtable", "outreg", "glmnet", "ggplot2", "ggfortify")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
install.packages(x)
})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
#read in data: Please set your working directory!
setwd("C:/Users/Tammena/Documents/SPL-HousePriceRegression/quantlets/Regression_Models")
df    = read.csv("train_preprocessed.csv")
#set rownumbers in dataframe to NULL
df$X  = NULL
df$Id = NULL
#1. Linear Model
#function, that starts with all vars and keeps only significant ones until every var is
#significant
sign.select = function(dframe, y) {
pvals          = 1
z              = 1
i              = 1
vars.selection = names(dframe)
vars.selection = vars.selection[!vars.selection %in% y]
while (z > 0) {
df.lm          = cbind(dframe[vars.selection], dframe[y])
lm1            = lm(formula(paste(y, "~ . ")), data = df.lm)
pvals          = summary(lm1)$coefficients[, 4]
pvals          = pvals[!names(pvals) %in% "(Intercept)"]
vars.selection = names(pvals[pvals < 0.05])
z              = sum(pvals > 0.05)
print(vars.selection)
i = i + 1
if (i == 300) {
warning("Did not finish in 300 iterations. No significant variables in data set?")
break
}
}
return(vars.selection)
}
vars   = sign.select(df, "logSalePrice")
lm.fit = lm(logSalePrice ~ ., data = df[, c(vars, "logSalePrice")])
summary(lm.fit)
#2. forward stepwise regression based on AIC
all.fit  =  formula(lm(logSalePrice ~ ., data = df))
none.fit = lm(logSalePrice ~ 1, data = df)
fwd.fit  = step(none.fit, direction='forward', scope=all.fit, trace=TRUE)
#plot the AIC values vs. No. Variables
AIC = fwd.fit$anova$AIC
VAR = row_number(-AIC)
gg  = as.data.frame(cbind(VAR, AIC))
png(file = "step.png", width = 800, height = 800)
ggplot(aes(x=VAR, y = AIC), data = gg)+ geom_point() + theme_classic()
dev.off()
# 3. Ridge and Lasso
#vars for ridge and lasso
y = as.matrix(df$logSalePrice)
x = as.matrix(df[!names(df) %in% c("logSalePrice")])
#ridge and lasso regression function to estimate the optimal penalty parameter lambda
#with 10-fold cross validation
lm.penal = function(type, x, y) {
if (type == "lasso") {
alpha = 1
} else if ( type == "ridge") {
alpha = 0
} else
stop("type must be either ridge or lasso")
cvfit          = cv.glmnet(x, y, alpha = alpha, nfolds = 10)
fit            =  predict(cvfit,newx=x, s="lambda.1se")
sst            = sum(y^2)
sse            = sum((fit - y)^2)
# R squared
rsq            = 1 - sse / sst
c              = coef(cvfit, s = "lambda.1se")
inds           = which(c != 0)
variables      = row.names(c)[inds]
vars.selection = variables[!variables %in% "(Intercept)"]
coeftable      = data.frame(var = variables,
coeff            = c[inds],
stringsAsFactors = FALSE)
c              = round(c, digits = 3)
rsq            = round(rsq, digits = 2)
output         = list(vars.selection, coeftable, c, cvfit, fit, rsq)
}
#perform regressions
lasso = lm.penal(type = "lasso", x = x, y = y)
ridge = lm.penal(type = "ridge", x = x, y = y)
#plot the optimal lamdba for lasso
png(file = "lasso_lambda.png", width = 800, height = 800)
autoplot(lasso[[4]]) + theme_classic() + theme(panel.background = element_rect(fill='white', color="black"))
dev.off()
#plot the lasso penalty results
lasso.plot = lm.penal(type="lasso", x = as.matrix(df[, lasso[[1]]]), y = y)
png(file = "lasso.png", width = 1200, height = 800)
autoplot(lasso.plot[[4]]$glmnet.fit, xvar="lambda") + theme_classic()
dev.off()
#make a table for latex
#dummy table that includes all vars
dummy     = lm(logSalePrice~., data=df)
table.out = outreg(setNames(list(lm.fit, fwd.fit, dummy, dummy), c("Sign. Selec.", "AIC Selec.", "Lasso", "Ridge")), se = FALSE)
#cut away unnecessary stats
table.out = table.out [1:180,]
#replace dummycoeffs
table.out[,"Lasso"]     = as.character(c(as.vector(lasso[[3]]), nrow(df), lasso[[6]]))
table.out[,"Ridge"]     = as.character(c(as.vector(ridge[[3]]), nrow(df), ridge[[6]]))
table.out[table.out==0] = ""
table.out               = rbind(table.out, c("", "Number Vars", length(lm.fit$coefficients),  length(fwd.fit$coefficients), length(lasso[[1]]),
length(ridge[[1]])))
table.x                 = xtable(table.out[c(1:7, 13:19, 177:181), ], caption = "Excerpt of Regression Model Results")
print(table.x, type = "latex", file = "reg_table.tex",include.rownames = FALSE)
#save R objects for further analysis
lasso.fit = lasso[[4]]
ridge.fit = ridge[[4]]
objects   = c("lasso.fit", "ridge.fit", "lm.fit", "fwd.fit")
save(list = objects, file = "regression_models_fit.RData")
workdir = "C:/Users/Tammena/Documents/SPL-HousePriceRegression/quantlets/"
d_init = yaml.debugger.init(workdir, show_keywords = TRUE)
qnames = yaml.debugger.get.qnames(d_init$RootPath)
d_results = yaml.debugger.run(qnames, d_init)
OverView = yaml.debugger.summary(qnames, d_results, summaryType = "mini")
# Install and load packages
libraries = c("ggplot2", "xtable")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
install.packages(x)
})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
# Read in dataing Data:
data = read.csv("train.csv")
# Analysis of Target Variable
# Histogram of Target Variable
price.hist = ggplot(data, aes(x = SalePrice)) + geom_histogram(aes(y = ..density..), bins = 20,
colour = "black", fill = "white") + geom_density(alpha = 0.2, fill = "#FF6666") + ggtitle("Distribution of Sale Price vs. Normal Distribution") +
xlab("Sale Price") + stat_function(fun = dnorm, args = list(mean = mean(data$SalePrice,
na.rm = TRUE), sd = sd(data$SalePrice, na.rm = TRUE)), col = "blue", size = 2) + theme_classic()
ggsave(filename = "PriceHist.pdf")
# Histogram and QQ Plot of Log Prices
logprice.hist = ggplot(data, aes(x = log(SalePrice))) + geom_histogram(aes(y = ..density..),
bins = 20, colour = "black", fill = "white") + geom_density(alpha = 0.2, fill = "#FF6666") +
ggtitle("Distribution of  Log Sale Price vs. Normal Distribution") + xlab("Log Sale Price") +
stat_function(fun = dnorm, args = list(mean = mean(log(data$SalePrice), na.rm = TRUE), sd = sd(log(data$SalePrice),
na.rm = TRUE)), col = "blue", size = 2)  + theme_classic()
ggsave(filename = "LogPriceHist.pdf")
# Standardised Log Prices
stdprice.hist = ggplot(data, aes(x = (log(SalePrice) - mean(log(SalePrice), na.rm = TRUE))/sd(log(SalePrice),
na.rm = TRUE))) + geom_histogram(aes(y = ..density..), bins = 20, colour = "black", fill = "white") +
geom_density(alpha = 0.2, fill = "#FF6666") + ggtitle("Distribution of Standardised Log Sale Price vs. Normal Distribution") +
xlab("Standardised Log Sale Price") + stat_function(fun = dnorm, col = "blue", size = 2) + theme_classic()
ggsave(filename = "StdPriceHist.pdf")
stdprice.qq = ggplot(data, aes(sample = (log(SalePrice) - mean(log(SalePrice), na.rm = TRUE))/sd(log(SalePrice),
na.rm = TRUE))) + stat_qq() + geom_abline(slope = 1, intercept = 0, col = "red") + ggtitle("QQ-Plot of Standardised Log Sale Price") +
xlab("Standardised Log Sale Price")  + theme_classic()
ggsave(filename = "StdPriceQQ.pdf")
# Get Column Classes:
colclasses = sapply(data, class)
table(colclasses)
# Seperate data in numeric and categoric variables for further analysis
numeric.data = data[, names(colclasses[colclasses != "factor"])]
categoric.data = data[, names(colclasses[colclasses == "factor"])]
# Drop Id Variable since it is unnecessary here
numeric.data$Id = NULL
# Exclude Target Variable
numeric.data$SalePrice = NULL
# Define Functions for variable overview
getmode = function(x) {
x = x[!is.na(x)]
unique(x)[which.max(tabulate(match(x, unique(x))))]
}
getmodefreq = function(x) {
mean(x == getmode(x), na.rm = TRUE)
}
getlevelcount = function(x) {
x = x[!is.na(x)]
length(unique(x))
}
#These are just wrappers around existing functions setting na.rm = TRUE
meanwrapper       = function(x) mean(x, na.rm = TRUE)
medianwrapper     = function(x) median(x, na.rm = TRUE)
sdwrapper         = function(x) sd(x, na.rm = TRUE)
# Create Overview table for categoric variables:
categoric.overview = data.frame(NACount = colSums(sapply(categoric.data, is.na)), LevelCount = sapply(categoric.data,
FUN = getlevelcount), Mode = sapply(categoric.data, FUN = getmode), ModeFrequency = sapply(categoric.data,
FUN = getmodefreq))
categoric.overview_latex = xtable(categoric.overview)
print(categoric.overview_latex, file = "categoric.overview.tex")
# Export Categorical Overview as Latex Table
rows.per.table = 30
row.indices = seq(from = 1, to = nrow(categoric.overview), by = rows.per.table)
latex.vector = character(0)
for(i in 1:length(row.indices)){
cap = paste0("Overview Categorical Variables Table:",i)
lab = paste0("tab:categoric.overview",i)
categoric.overview_latex = xtable(categoric.overview[row.indices[i]:min(row.indices[i] + rows.per.table - 1, nrow(categoric.overview)),,drop = FALSE], caption = cap, label =lab)
latex.vector = c(latex.vector, print(categoric.overview_latex))
}
all.latex = paste(latex.vector, collapse = "\n")
writeLines(all.latex, con = "categoric_overview.tex")
# Create Overview table for numericvariables Create Overview table for categoric variables:
numeric.overview = data.frame(NACount = colSums(sapply(numeric.data, is.na)), LevelCount = sapply(numeric.data,
FUN = getlevelcount), Mode = sapply(numeric.data, FUN = getmode), ModeFrequency = sapply(numeric.data,
FUN = getmodefreq), Mean = sapply(numeric.data, FUN = meanwrapper), Median = sapply(numeric.data,
FUN = medianwrapper), SD = sapply(numeric.data, FUN = sdwrapper))
# Export Categorical Overview as Latex Table
rows.per.table = 30
row.indices = seq(from = 1, to = nrow(numeric.overview), by = rows.per.table)
latex.vector = character(0)
for(i in 1:length(row.indices)){
cap = paste0("Overview Numeric Variables Table:",i)
lab = paste0("tab:numeric.overview",i)
numeric.overview_latex = xtable(numeric.overview[row.indices[i]:min(row.indices[i] + rows.per.table - 1, nrow(numeric.overview)),,drop = FALSE], caption = cap, label =lab)
latex.vector = c(latex.vector, print(numeric.overview_latex))
}
all.latex = paste(latex.vector, collapse = "\n")
writeLines(all.latex, con = "numeric_overview.tex")
#clear variables and close windows
rm(list = ls(all = TRUE))
graphics.off()
#install and load packages
libraries = c("xtable", "outreg", "glmnet", "ggplot2", "ggfortify")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
install.packages(x)
})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
#read in data: Please set your working directory!
setwd("C:/Users/Tammena/Documents/SPL-HousePriceRegression/quantlets/Regression_Models")
df    = read.csv("train_preprocessed.csv")
#set rownumbers in dataframe to NULL
df$X  = NULL
df$Id = NULL
#1. Linear Model
#function, that starts with all vars and keeps only significant ones until every var is
#significant
sign.select = function(dframe, y) {
pvals          = 1
z              = 1
i              = 1
vars.selection = names(dframe)
vars.selection = vars.selection[!vars.selection %in% y]
while (z > 0) {
df.lm          = cbind(dframe[vars.selection], dframe[y])
lm1            = lm(formula(paste(y, "~ . ")), data = df.lm)
pvals          = summary(lm1)$coefficients[, 4]
pvals          = pvals[!names(pvals) %in% "(Intercept)"]
vars.selection = names(pvals[pvals < 0.05])
z              = sum(pvals > 0.05)
print(vars.selection)
i = i + 1
if (i == 300) {
warning("Did not finish in 300 iterations. No significant variables in data set?")
break
}
}
return(vars.selection)
}
vars   = sign.select(df, "logSalePrice")
lm.fit = lm(logSalePrice ~ ., data = df[, c(vars, "logSalePrice")])
summary(lm.fit)
#2. forward stepwise regression based on AIC
all.fit  =  formula(lm(logSalePrice ~ ., data = df))
none.fit = lm(logSalePrice ~ 1, data = df)
fwd.fit  = step(none.fit, direction='forward', scope=all.fit, trace=TRUE)
#plot the AIC values vs. No. Variables
AIC = fwd.fit$anova$AIC
VAR = row_number(-AIC)
gg  = as.data.frame(cbind(VAR, AIC))
png(file = "step.png", width = 800, height = 800)
ggplot(aes(x=VAR, y = AIC), data = gg)+ geom_point() + theme_classic()
dev.off()
# 3. Ridge and Lasso
#vars for ridge and lasso
y = as.matrix(df$logSalePrice)
x = as.matrix(df[!names(df) %in% c("logSalePrice")])
#ridge and lasso regression function to estimate the optimal penalty parameter lambda
#with 10-fold cross validation
lm.penal = function(type, x, y) {
if (type == "lasso") {
alpha = 1
} else if ( type == "ridge") {
alpha = 0
} else
stop("type must be either ridge or lasso")
cvfit          = cv.glmnet(x, y, alpha = alpha, nfolds = 10)
fit            =  predict(cvfit,newx=x, s="lambda.1se")
sst            = sum(y^2)
sse            = sum((fit - y)^2)
# R squared
rsq            = 1 - sse / sst
c              = coef(cvfit, s = "lambda.1se")
inds           = which(c != 0)
variables      = row.names(c)[inds]
vars.selection = variables[!variables %in% "(Intercept)"]
coeftable      = data.frame(var = variables,
coeff            = c[inds],
stringsAsFactors = FALSE)
c              = round(c, digits = 3)
rsq            = round(rsq, digits = 2)
output         = list(vars.selection, coeftable, c, cvfit, fit, rsq)
}
#perform regressions
lasso = lm.penal(type = "lasso", x = x, y = y)
ridge = lm.penal(type = "ridge", x = x, y = y)
#plot the optimal lamdba for lasso
png(file = "lasso_lambda.png", width = 800, height = 800)
autoplot(lasso[[4]]) + theme_classic() + theme(panel.background = element_rect(fill='white', color="black"))
dev.off()
#plot the lasso penalty results
lasso.plot = lm.penal(type="lasso", x = as.matrix(df[, lasso[[1]]]), y = y)
png(file = "lasso.png", width = 1200, height = 800)
autoplot(lasso.plot[[4]]$glmnet.fit, xvar="lambda") + theme_classic()
dev.off()
#make a table for latex
#dummy table that includes all vars
dummy     = lm(logSalePrice~., data=df)
table.out = outreg(setNames(list(lm.fit, fwd.fit, dummy, dummy), c("Sign. Selec.", "AIC Selec.", "Lasso", "Ridge")), se = FALSE)
#cut away unnecessary stats
table.out = table.out [1:180,]
#replace dummycoeffs
table.out[,"Lasso"]     = as.character(c(as.vector(lasso[[3]]), nrow(df), lasso[[6]]))
table.out[,"Ridge"]     = as.character(c(as.vector(ridge[[3]]), nrow(df), ridge[[6]]))
table.out[table.out==0] = ""
table.out               = rbind(table.out, c("", "Number Vars", length(lm.fit$coefficients),  length(fwd.fit$coefficients), length(lasso[[1]]),
length(ridge[[1]])))
table.x                 = xtable(table.out[c(1:7, 13:19, 177:181), ], caption = "Excerpt of Regression Model Results")
print(table.x, type = "latex", file = "reg_table.tex",include.rownames = FALSE)
#save R objects for further analysis
lasso.fit = lasso[[4]]
ridge.fit = ridge[[4]]
objects   = c("lasso.fit", "ridge.fit", "lm.fit", "fwd.fit")
save(list = objects, file = "regression_models_fit.RData")
#clear variables and close windows
rm(list = ls(all = TRUE))
graphics.off()
#install and load packages
libraries = c("xtable", "outreg", "glmnet", "ggplot2", "ggfortify")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
install.packages(x)
})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
#read in data: Please set your working directory!
setwd("C:/Users/Tammena/Documents/SPL-HousePriceRegression/quantlets/Regression_Models")
df    = read.csv("train_preprocessed.csv")
#set rownumbers in dataframe to NULL
df$X  = NULL
df$Id = NULL
#1. Linear Model
#function, that starts with all vars and keeps only significant ones until every var is
#significant
sign.select = function(dframe, y) {
pvals          = 1
z              = 1
i              = 1
vars.selection = names(dframe)
vars.selection = vars.selection[!vars.selection %in% y]
while (z > 0) {
df.lm          = cbind(dframe[vars.selection], dframe[y])
lm1            = lm(formula(paste(y, "~ . ")), data = df.lm)
pvals          = summary(lm1)$coefficients[, 4]
pvals          = pvals[!names(pvals) %in% "(Intercept)"]
vars.selection = names(pvals[pvals < 0.05])
z              = sum(pvals > 0.05)
print(vars.selection)
i = i + 1
if (i == 300) {
warning("Did not finish in 300 iterations. No significant variables in data set?")
break
}
}
return(vars.selection)
}
vars   = sign.select(df, "logSalePrice")
lm.fit = lm(logSalePrice ~ ., data = df[, c(vars, "logSalePrice")])
summary(lm.fit)
#2. forward stepwise regression based on AIC
all.fit  =  formula(lm(logSalePrice ~ ., data = df))
none.fit = lm(logSalePrice ~ 1, data = df)
fwd.fit  = step(none.fit, direction='forward', scope=all.fit, trace=TRUE)
#plot the AIC values vs. No. Variables
AIC = fwd.fit$anova$AIC
VAR = row_number(-AIC)
gg  = as.data.frame(cbind(VAR, AIC))
png(file = "step.png", width = 800, height = 800)
ggplot(aes(x=VAR, y = AIC), data = gg)+ geom_point() + theme_classic()
dev.off()
# 3. Ridge and Lasso
#vars for ridge and lasso
y = as.matrix(df$logSalePrice)
x = as.matrix(df[!names(df) %in% c("logSalePrice")])
#ridge and lasso regression function to estimate the optimal penalty parameter lambda
#with 10-fold cross validation
lm.penal = function(type, x, y) {
if (type == "lasso") {
alpha = 1
} else if ( type == "ridge") {
alpha = 0
} else
stop("type must be either ridge or lasso")
cvfit          = cv.glmnet(x, y, alpha = alpha, nfolds = 10)
fit            =  predict(cvfit,newx=x, s="lambda.1se")
sst            = sum(y^2)
sse            = sum((fit - y)^2)
# R squared
rsq            = 1 - sse / sst
c              = coef(cvfit, s = "lambda.1se")
inds           = which(c != 0)
variables      = row.names(c)[inds]
vars.selection = variables[!variables %in% "(Intercept)"]
coeftable      = data.frame(var = variables,
coeff            = c[inds],
stringsAsFactors = FALSE)
c              = round(c, digits = 3)
rsq            = round(rsq, digits = 2)
output         = list(vars.selection, coeftable, c, cvfit, fit, rsq)
}
#perform regressions
lasso = lm.penal(type = "lasso", x = x, y = y)
ridge = lm.penal(type = "ridge", x = x, y = y)
#plot the optimal lamdba for lasso
png(file = "lasso_lambda.png", width = 800, height = 800)
autoplot(lasso[[4]]) + theme_classic() + theme(panel.background = element_rect(fill='white', color="black"))
dev.off()
#plot the lasso penalty results
lasso.plot = lm.penal(type="lasso", x = as.matrix(df[, lasso[[1]]]), y = y)
png(file = "lasso.png", width = 1200, height = 800)
autoplot(lasso.plot[[4]]$glmnet.fit, xvar="lambda") + theme_classic()
dev.off()
#make a table for latex
#dummy table that includes all vars
dummy     = lm(logSalePrice~., data=df)
table.out = outreg(setNames(list(lm.fit, fwd.fit, dummy, dummy), c("Sign. Selec.", "AIC Selec.", "Lasso", "Ridge")), se = FALSE)
#cut away unnecessary stats
table.out = table.out [1:180,]
#replace dummycoeffs
table.out[,"Lasso"]     = as.character(c(as.vector(lasso[[3]]), nrow(df), lasso[[6]]))
table.out[,"Ridge"]     = as.character(c(as.vector(ridge[[3]]), nrow(df), ridge[[6]]))
table.out[table.out==0] = ""
table.out               = rbind(table.out, c("", "Number Vars", length(lm.fit$coefficients),  length(fwd.fit$coefficients), length(lasso[[1]]),
length(ridge[[1]])))
table.x                 = xtable(table.out[c(1:7, 13:19, 177:181), ], caption = "Excerpt of Regression Model Results")
print(table.x, type = "latex", file = "reg_table.tex",include.rownames = FALSE)
#save R objects for further analysis
lasso.fit = lasso[[4]]
ridge.fit = ridge[[4]]
objects   = c("lasso.fit", "ridge.fit", "lm.fit", "fwd.fit")
save(list = objects, file = "regression_models_fit.RData")
autoplot(lasso[[4]]) + theme_classic() + theme(panel.background = element_rect(fill='white', color="black"))
