#make 80/20 split
set.seed(123)
ids = sample(df.preprocessed$Id, nrow(df.preprocessed)*0.2)
write.csv(df.preprocessed[df.preprocessed$Id %in% ids,], file = "test_preprocessed.csv")
write.csv(df.preprocessed[!df.preprocessed$Id %in% ids,], file = "train_preprocessed.csv")
# clear variables and close windows
rm(list = ls(all = TRUE))
graphics.off()
# Install and load packages
libraries = c("ggplot2", "reshape2", "dplyr")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {install.packages(x)})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
#read in data: Please set your working directory!
setwd("C:/Users/Tammena/Documents/SPL-HousePriceRegression/quantlets/Data_Preprocessing")
# Read in data:
df = read.csv("train.csv")
#delete ID and take logs of sale price, see quantlet explanatory data analysis
df$logSalePrice = log(df$SalePrice)
df$SalePrice    = NULL
df$Id           = NULL
# 1. Missings and Imputation table with the number of NAs per Variable
na.summary = function(data) {
na_summary = sapply(data, function(x) sum(is.na(x)))
na_summary[na_summary > 0]
}
na.summary(df)
# some NA's do have a meaning, though
vars = c("Alley", "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2",
"FireplaceQu", "GarageType", "GarageFinish", "GarageQual", "GarageCond", "PoolQC",
"Fence", "MiscFeature")
# change NA to None
na2none = function(data, var) {
levels(data[, var]) = c(levels(data[, var]), "none")
data[, var][is.na(data[, var])] = "none"
return(data[, var])
}
for (i in 1:length(vars)) {
df[, vars[i]] = na2none(df, vars[i])
}
# Check the results of above function
na.summary(df)
# Plot a missingness map. Again the na.summary function
# comes in handy
na_vars = names(na.summary(df))
# make a function for a missingness plot with ggplot2
miss.plot = function(x) {
x     %>%
is.na %>%
melt  %>%
ggplot(data = ., aes(x = Var2, y = Var1))  + geom_raster(aes(fill = value))   + scale_fill_discrete(name = "", labels = c("Present", "Missing"))    + theme_classic()     + theme(axis.text.x = element_text(angle = 45, vjust = 0.5))  + labs(x = "Variables in Dataset", y = "Rows / observations")
}
png(file = "missmap.png", width = 800, height = 800)
miss.plot(df[na_vars])
dev.off()
# Select categorical and numerical variables
colclasses     = sapply(df[na_vars], class)
categoric.data = df[, names(colclasses[colclasses == "factor"])]
numeric.data   = df[, names(colclasses[colclasses != "factor"])]
# Impute numeric data with median
impute.median = function(x) {
nas    = is.na(x)
x[nas] = median(x[!nas])
as.numeric(x)
}
numeric.imputed = as.data.frame(sapply(numeric.data, impute.median))
# imputation for the categoric data we need a custom mode function, because R's built
# in works only for numeric data
Mode = function(x) {
ux = unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
impute.mode = function(x) {
nas    = is.na(x)
x[nas] = Mode(x[!nas])
as.factor(x)
}
categoric.imputed = as.data.frame(sapply(categoric.data, impute.mode))
# make new dataframe with non-imputed colums from df, and the imputed variables
df.imputed = cbind(df[, !(names(df) %in% names(cbind(categoric.imputed, numeric.imputed)))],
categoric.imputed, numeric.imputed)
# check if all missings are gone
anyNA(df.imputed)
# 2. merge factorlevels that are almost empty function to set categories with less
# than 20 observations to 'other'
single.factors = function(data) {
for (var in names(data)) {
if (is.factor(data[[var]])) {
tbl = table(data[[var]])
ren = names(tbl)[tbl <= 20]
# rename all matching levels to other
levels(data[[var]])[levels(data[[var]]) %in% ren] = "Other"
# same procedure again, if now there is still a category with less
# than 20 it can only be "other"!
tbl     = table(data[[var]])
tbl_sum = sum(tbl < 20)
if (nlevels(data[[var]]) < 3 & tbl_sum >= 1)
data[[var]] = NA
}
}
return(data)
}
# apply function and remove the NAs
df.merged = single.factors(df.imputed)
df.merged = df.merged[, colSums(is.na(df.merged)) != nrow(df.merged)]
# 3. Outliers find outliers in numeric data
colclasses      = sapply(df.merged, class)
df.temp.numeric = df.merged[, names(colclasses[colclasses != "factor"])]
# Make the function to identify extreme outliers (more than 3 times IQR)
outlier.count = function(x) {
low  = as.numeric(quantile(x)[2] - IQR(x) * 3)
high = as.numeric(IQR(x) * 3 + quantile(x)[4])
sum(x >= high | x <= low)
}
# make a table to see the count of outliers
outlier_table = sapply(df.temp.numeric, outlier.count)
outlier_table = outlier_table[outlier_table > 0]
outlier_table
# removing vars with IQR = 0 (the case where all vars are 'outliers'), because these
# variables will cause trouble wenn we split the dataset. also, 0 means that something
# doesnt exist which is coverd by the other vars
outlier_table[outlier_table == 1460]
df.temp.numeric[names(outlier_table[outlier_table == 1460])] = NULL
# function to return the number of unique values of a var
unique.values = function(var) {
un = unique(var)
length(un)
}
count_unique = sapply(df.temp.numeric, unique.values)
# vars with less than 10 distinct values, are they sensibly numeric?
less_than_10 = count_unique[count_unique < 10]
sapply(df.temp.numeric[names(less_than_10)], table)
# it looks OK!
# now cut the remaining outliers to 3.0 IQR distance
outlier.truncate = function(x) {
low         = as.numeric(quantile(x)[2] - IQR(x) * 3)
high        = as.numeric(IQR(x) * 3 + quantile(x)[4])
x[x < low]  = low
x[x > high] = high
print(x)
return(x)
}
df.outlier.trunc = as.data.frame(sapply(df.temp.numeric, outlier.truncate))
# make a number of boxplots for some vars with many outliers for comparision
df.plot           = rbind(df.temp.numeric[, names(outlier_table[outlier_table<1460& outlier_table>5])], df.outlier.trunc[, names(outlier_table[outlier_table<1460& outlier_table>5])])
df.plot$truncated = as.factor(c(rep(0, nrow(df.temp.numeric)), rep(1, nrow(df.outlier.trunc))))
gg                = melt(df.plot, id="truncated")
png(file = "boxplots.png", width = 1200, height = 800)
ggplot(data = gg, aes(x=variable, y=value)) + geom_boxplot(aes(fill=truncated))  + facet_wrap( ~ variable, scales="free")  + theme_classic()
dev.off()
# 4. Plot the cleaned dataset make a number of histograms
gg = melt(scale(df.outlier.trunc[, !names(df.outlier.trunc) %in% c("Id")]))
png(file = "histograms.png", width = 1200, height = 800)
qplot(data = gg, x = value, facets = ~Var2, bins = 30) + theme_classic()
dev.off()
# 5. Reduction of dimensionality of numeric variables only select those vars, that
# have a reasonable correlation
cormat = cor(df.outlier.trunc[, !names(df.outlier.trunc) %in% c("logSalePrice")])
list   = which(abs(cormat) > 0.7 & abs(cormat) < 1, arr.ind = TRUE)
df.pca = df.outlier.trunc[unique(rownames(list))]
cor(df.pca)
prin1 = princomp(df.pca, cor = T, scores = T)
png(file = "screeplot.png", width = 800, height = 800)
screeplot(prin1, type = "l")
dev.off()
summary(prin1)
# we use 4 components, to explain > 70% of variance
scores = prin1$scores[, 1:4]
#to be able to select factor levels automatically, convert all factors into dummies
mm = model.matrix(~. - 1, data = df.merged[, names(colclasses[colclasses == "factor"])])
# make dataframe from remaining numeric vars and PCA-Results and dummy vars
df.preprocessed = cbind(df.outlier.trunc[, !names(df.outlier.trunc) %in% rownames(list)], scores, mm)
#avoid spaces in names
df.preprocessed        = as.data.frame(df.preprocessed)
names(df.preprocessed) = make.names(names(df.preprocessed), unique = TRUE)
#standardize numerics
df.preprocessed[!names(df.preprocessed) %in% colnames(mm)]= scale(df.preprocessed[!names(df.preprocessed) %in% colnames(mm)])
summary(df.preprocessed)
#split into test and trainingsdata
df.preprocessed$Id = 1:nrow(df.preprocessed)
#make 80/20 split
set.seed(123)
ids = sample(df.preprocessed$Id, nrow(df.preprocessed)*0.2)
write.csv(df.preprocessed[df.preprocessed$Id %in% ids,], file = "test_preprocessed.csv")
write.csv(df.preprocessed[!df.preprocessed$Id %in% ids,], file = "train_preprocessed.csv")
png(file = "missmap.png", width = 800, height = 800, pointsize=12)
miss.plot(df[na_vars])
dev.off()
png(file = "missmap.png", width = 12, height = 12, units='cm', pointsize=12)
miss.plot(df[na_vars])
dev.off()
png(file = "missmap.png", width = 800, height = 800, pointsize=16)
miss.plot(df[na_vars])
dev.off()
png(file = "missmap.png", width = 1200, height = 1200, pointsize=16)
miss.plot(df[na_vars])
dev.off()
png(file = "missmap.png", width = 1200, height = 1200, pointsize=36)
miss.plot(df[na_vars])
dev.off()
png(file = "missmap.png", width = 12, height = 12, unit='cm')
miss.plot(df[na_vars])
dev.off()
png(file = "missmap.png", width = 12, height = 12, unit='cm', res = 100)
miss.plot(df[na_vars])
dev.off()
png(file = "missmap.png", width = 16, height = 16, unit='cm', res = 100)
miss.plot(df[na_vars])
dev.off()
png(file = "missmap.png", width = 16, height = 16, unit='cm', res = 120)
miss.plot(df[na_vars])
dev.off()
png(file = "missmap.png", width = 16, height = 16, unit='cm', res = 72)
miss.plot(df[na_vars])
dev.off()
png(file = "missmap.png", width = 16, height = 16, unit='cm', res = 100)
miss.plot(df[na_vars])
dev.off()
# clear variables and close windows
rm(list = ls(all = TRUE))
graphics.off()
# Install and load packages
libraries = c("ggplot2", "reshape2", "dplyr")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {install.packages(x)})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
#read in data: Please set your working directory!
setwd("C:/Users/Tammena/Documents/SPL-HousePriceRegression/quantlets/Data_Preprocessing")
# Read in data:
df = read.csv("train.csv")
#delete ID and take logs of sale price, see quantlet explanatory data analysis
df$logSalePrice = log(df$SalePrice)
df$SalePrice    = NULL
df$Id           = NULL
# 1. Missings and Imputation table with the number of NAs per Variable
na.summary = function(data) {
na_summary = sapply(data, function(x) sum(is.na(x)))
na_summary[na_summary > 0]
}
na.summary(df)
# some NA's do have a meaning, though
vars = c("Alley", "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2",
"FireplaceQu", "GarageType", "GarageFinish", "GarageQual", "GarageCond", "PoolQC",
"Fence", "MiscFeature")
# change NA to None
na2none = function(data, var) {
levels(data[, var]) = c(levels(data[, var]), "none")
data[, var][is.na(data[, var])] = "none"
return(data[, var])
}
for (i in 1:length(vars)) {
df[, vars[i]] = na2none(df, vars[i])
}
# Check the results of above function
na.summary(df)
# Plot a missingness map. Again the na.summary function
# comes in handy
na_vars = names(na.summary(df))
# make a function for a missingness plot with ggplot2
miss.plot = function(x) {
x     %>%
is.na %>%
melt  %>%
ggplot(data = ., aes(x = Var2, y = Var1))  + geom_raster(aes(fill = value))   + scale_fill_discrete(name = "", labels = c("Present", "Missing"))    + theme_classic()     + theme(axis.text.x = element_text(angle = 45, vjust = 0.5))  + labs(x = "Variables in Dataset", y = "Rows / observations")
}
png(file = "missmap.png", width = 16, height = 16, unit='cm', res = 100)
miss.plot(df[na_vars])
dev.off()
# Select categorical and numerical variables
colclasses     = sapply(df[na_vars], class)
categoric.data = df[, names(colclasses[colclasses == "factor"])]
numeric.data   = df[, names(colclasses[colclasses != "factor"])]
# Impute numeric data with median
impute.median = function(x) {
nas    = is.na(x)
x[nas] = median(x[!nas])
as.numeric(x)
}
numeric.imputed = as.data.frame(sapply(numeric.data, impute.median))
# imputation for the categoric data we need a custom mode function, because R's built
# in works only for numeric data
Mode = function(x) {
ux = unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
impute.mode = function(x) {
nas    = is.na(x)
x[nas] = Mode(x[!nas])
as.factor(x)
}
categoric.imputed = as.data.frame(sapply(categoric.data, impute.mode))
# make new dataframe with non-imputed colums from df, and the imputed variables
df.imputed = cbind(df[, !(names(df) %in% names(cbind(categoric.imputed, numeric.imputed)))],
categoric.imputed, numeric.imputed)
# check if all missings are gone
anyNA(df.imputed)
# 2. merge factorlevels that are almost empty function to set categories with less
# than 20 observations to 'other'
single.factors = function(data) {
for (var in names(data)) {
if (is.factor(data[[var]])) {
tbl = table(data[[var]])
ren = names(tbl)[tbl <= 20]
# rename all matching levels to other
levels(data[[var]])[levels(data[[var]]) %in% ren] = "Other"
# same procedure again, if now there is still a category with less
# than 20 it can only be "other"!
tbl     = table(data[[var]])
tbl_sum = sum(tbl < 20)
if (nlevels(data[[var]]) < 3 & tbl_sum >= 1)
data[[var]] = NA
}
}
return(data)
}
# apply function and remove the NAs
df.merged = single.factors(df.imputed)
df.merged = df.merged[, colSums(is.na(df.merged)) != nrow(df.merged)]
# 3. Outliers find outliers in numeric data
colclasses      = sapply(df.merged, class)
df.temp.numeric = df.merged[, names(colclasses[colclasses != "factor"])]
# Make the function to identify extreme outliers (more than 3 times IQR)
outlier.count = function(x) {
low  = as.numeric(quantile(x)[2] - IQR(x) * 3)
high = as.numeric(IQR(x) * 3 + quantile(x)[4])
sum(x >= high | x <= low)
}
# make a table to see the count of outliers
outlier_table = sapply(df.temp.numeric, outlier.count)
outlier_table = outlier_table[outlier_table > 0]
outlier_table
# removing vars with IQR = 0 (the case where all vars are 'outliers'), because these
# variables will cause trouble wenn we split the dataset. also, 0 means that something
# doesnt exist which is coverd by the other vars
outlier_table[outlier_table == 1460]
df.temp.numeric[names(outlier_table[outlier_table == 1460])] = NULL
# function to return the number of unique values of a var
unique.values = function(var) {
un = unique(var)
length(un)
}
count_unique = sapply(df.temp.numeric, unique.values)
# vars with less than 10 distinct values, are they sensibly numeric?
less_than_10 = count_unique[count_unique < 10]
sapply(df.temp.numeric[names(less_than_10)], table)
# it looks OK!
# now cut the remaining outliers to 3.0 IQR distance
outlier.truncate = function(x) {
low         = as.numeric(quantile(x)[2] - IQR(x) * 3)
high        = as.numeric(IQR(x) * 3 + quantile(x)[4])
x[x < low]  = low
x[x > high] = high
print(x)
return(x)
}
df.outlier.trunc = as.data.frame(sapply(df.temp.numeric, outlier.truncate))
# make a number of boxplots for some vars with many outliers for comparision
df.plot           = rbind(df.temp.numeric[, names(outlier_table[outlier_table<1460& outlier_table>5])], df.outlier.trunc[, names(outlier_table[outlier_table<1460& outlier_table>5])])
df.plot$truncated = as.factor(c(rep(0, nrow(df.temp.numeric)), rep(1, nrow(df.outlier.trunc))))
gg                = melt(df.plot, id="truncated")
png(file = "boxplots.png", width = 24, height = 16, unit='cm', res = 100)
ggplot(data = gg, aes(x=variable, y=value)) + geom_boxplot(aes(fill=truncated))  + facet_wrap( ~ variable, scales="free")  + theme_classic()
dev.off()
# 4. Plot the cleaned dataset make a number of histograms
gg = melt(scale(df.outlier.trunc[, !names(df.outlier.trunc) %in% c("Id")]))
png(file = "histograms.png", width = 24, height = 16, unit='cm', res = 100)
qplot(data = gg, x = value, facets = ~Var2, bins = 30) + theme_classic()
dev.off()
# 5. Reduction of dimensionality of numeric variables only select those vars, that
# have a reasonable correlation
cormat = cor(df.outlier.trunc[, !names(df.outlier.trunc) %in% c("logSalePrice")])
list   = which(abs(cormat) > 0.7 & abs(cormat) < 1, arr.ind = TRUE)
df.pca = df.outlier.trunc[unique(rownames(list))]
cor(df.pca)
# principal component analysis
prin1 = princomp(df.pca, cor = T, scores = T)
png(file = "screeplot.png", width = 16, height = 16, unit='cm', res = 100)
screeplot(prin1, type = "l")
dev.off()
summary(prin1)
# we use 4 components, to explain > 70% of variance
scores = prin1$scores[, 1:4]
#to be able to select factor levels automatically, convert all factors into dummies
mm = model.matrix(~. - 1, data = df.merged[, names(colclasses[colclasses == "factor"])])
# make dataframe from remaining numeric vars and PCA-Results and dummy vars
df.preprocessed = cbind(df.outlier.trunc[, !names(df.outlier.trunc) %in% rownames(list)], scores, mm)
#avoid spaces in names
df.preprocessed        = as.data.frame(df.preprocessed)
names(df.preprocessed) = make.names(names(df.preprocessed), unique = TRUE)
#standardize numerics
df.preprocessed[!names(df.preprocessed) %in% colnames(mm)]= scale(df.preprocessed[!names(df.preprocessed) %in% colnames(mm)])
summary(df.preprocessed)
#split into test and trainingsdata
df.preprocessed$Id = 1:nrow(df.preprocessed)
#make 80/20 split
set.seed(123)
ids = sample(df.preprocessed$Id, nrow(df.preprocessed)*0.2)
write.csv(df.preprocessed[df.preprocessed$Id %in% ids,], file = "test_preprocessed.csv")
write.csv(df.preprocessed[!df.preprocessed$Id %in% ids,], file = "train_preprocessed.csv")
#clear variables and close windows
rm(list = ls(all = TRUE))
graphics.off()
#install and load packages
libraries = c("xtable", "outreg", "glmnet", "ggplot2", "ggfortify")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
install.packages(x)
})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
#read in data: Please set your working directory!
setwd("C:/Users/Tammena/Documents/SPL-HousePriceRegression/quantlets/Regression_Models")
df    = read.csv("train_preprocessed.csv")
#set rownumbers in dataframe to NULL
df$X  = NULL
df$Id = NULL
#1. Linear Model
#function, that starts with all vars and keeps only significant ones until every var is
#significant
sign.select = function(dframe, y) {
pvals          = 1
z              = 1
i              = 1
vars.selection = names(dframe)
vars.selection = vars.selection[!vars.selection %in% y]
while (z > 0) {
df.lm          = cbind(dframe[vars.selection], dframe[y])
lm1            = lm(formula(paste(y, "~ . ")), data = df.lm)
pvals          = summary(lm1)$coefficients[, 4]
pvals          = pvals[!names(pvals) %in% "(Intercept)"]
vars.selection = names(pvals[pvals < 0.05])
z              = sum(pvals > 0.05)
print(vars.selection)
i = i + 1
if (i == 300) {
warning("Did not finish in 300 iterations. No significant variables in data set?")
break
}
}
return(vars.selection)
}
vars   = sign.select(df, "logSalePrice")
lm.fit = lm(logSalePrice ~ ., data = df[, c(vars, "logSalePrice")])
summary(lm.fit)
#2. forward stepwise regression based on AIC
all.fit  =  formula(lm(logSalePrice ~ ., data = df))
none.fit = lm(logSalePrice ~ 1, data = df)
fwd.fit  = step(none.fit, direction='forward', scope=all.fit, trace=TRUE)
#plot the AIC values vs. No. Variables
AIC = fwd.fit$anova$AIC
VAR = row_number(-AIC)
gg  = as.data.frame(cbind(VAR, AIC))
png(file = "step.png", width = 16, height = 16, unit='cm', res = 100)
ggplot(aes(x=VAR, y = AIC), data = gg)+ geom_point() + theme_classic()
dev.off()
# 3. Ridge and Lasso
#vars for ridge and lasso
y = as.matrix(df$logSalePrice)
x = as.matrix(df[!names(df) %in% c("logSalePrice")])
#ridge and lasso regression function to estimate the optimal penalty parameter lambda
#with 10-fold cross validation
lm.penal = function(type, x, y) {
if (type == "lasso") {
alpha = 1
} else if ( type == "ridge") {
alpha = 0
} else
stop("type must be either ridge or lasso")
cvfit          = cv.glmnet(x, y, alpha = alpha, nfolds = 10)
fit            =  predict(cvfit,newx=x, s="lambda.1se")
sst            = sum(y^2)
sse            = sum((fit - y)^2)
# R squared
rsq            = 1 - sse / sst
c              = coef(cvfit, s = "lambda.1se")
inds           = which(c != 0)
variables      = row.names(c)[inds]
vars.selection = variables[!variables %in% "(Intercept)"]
coeftable      = data.frame(var = variables,
coeff            = c[inds],
stringsAsFactors = FALSE)
c              = round(c, digits = 3)
rsq            = round(rsq, digits = 2)
output         = list(vars.selection, coeftable, c, cvfit, fit, rsq)
}
#perform regressions
lasso = lm.penal(type = "lasso", x = x, y = y)
ridge = lm.penal(type = "ridge", x = x, y = y)
#plot the optimal lamdba for lasso
png(file = "lasso_lambda.png", width = 16, height = 16, unit='cm', res = 100)
autoplot(lasso[[4]]) + theme_classic() + theme(panel.background = element_rect(fill='white', color="black"))
dev.off()
#plot the lasso penalty results
lasso.plot = lm.penal(type="lasso", x = as.matrix(df[, lasso[[1]]]), y = y)
png(file = "lasso.png", width = 24, height = 16, unit='cm', res = 100)
autoplot(lasso.plot[[4]]$glmnet.fit, xvar="lambda") + theme_classic()
dev.off()
#make a table for latex
#dummy table that includes all vars
dummy     = lm(logSalePrice~., data=df)
table.out = outreg(setNames(list(lm.fit, fwd.fit, dummy, dummy), c("Sign. Selec.", "AIC Selec.", "Lasso", "Ridge")), se = FALSE)
#cut away unnecessary stats
table.out = table.out [1:180,]
#replace dummycoeffs
table.out[,"Lasso"]     = as.character(c(as.vector(lasso[[3]]), nrow(df), lasso[[6]]))
table.out[,"Ridge"]     = as.character(c(as.vector(ridge[[3]]), nrow(df), ridge[[6]]))
table.out[table.out==0] = ""
table.out               = rbind(table.out, c("", "Number Vars", length(lm.fit$coefficients),  length(fwd.fit$coefficients), length(lasso[[1]]),
length(ridge[[1]])))
table.x                 = xtable(table.out[c(1:7, 13:19, 177:181), ], caption = "Excerpt of Regression Model Results")
print(table.x, type = "latex", file = "reg_table.tex",include.rownames = FALSE)
#save R objects for further analysis
lasso.fit = lasso[[4]]
ridge.fit = ridge[[4]]
objects   = c("lasso.fit", "ridge.fit", "lm.fit", "fwd.fit")
save(list = objects, file = "regression_models_fit.RData")
