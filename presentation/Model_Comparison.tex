\section{Model Comparison}

% 6-1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{Quantlet: Model Comparison}
\begin{itemize}
\item aim of this quantlet: after building and training all the models on the training data the goal is to measure how they perform on new data (=test data) 
\item computing different measures and show results graphically
\begin{itemize}
\item Mean Squared Error	
\item Bias  
\item real Vs. predicted plots 

\end{itemize}

\end{itemize}
}
% 6-2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Real Vs. Predicted: Code}

\begin{lstlisting}[basicstyle=\tiny][language=R]
predictions.lm = predict(lm.fit, newdata = test)
df.lm.fit      = data.frame(cbind(test$logSalePrice, predictions.lm ))

lm.plot = ggplot(df.lm.fit, aes(test$logSalePrice, predictions.lm)) + geom_point() + geom_segment(x = -4, y = -4, xend = 4, yend = 4, color = "red", size = 1.3) + 
    stat_smooth(method = "lm", se = FALSE) + 
    labs(title = "Plot of real logSalePrice against predicted values", 
    x = "logSalePrice", y = "lm.fit predictions") + theme(axis.title = element_text(size = 16), plot.title = element_text(size = 16, 
    face = "bold")) + 
    annotate("text", label = paste("MSE:", comparison.result["MSE", "lm"], sep = " "), x = -3, y = 3) + 
    annotate("text", label = paste("MAE:", comparison.result["MAE", "lm"], sep = " "), x = -3, y = 2.5)

\end{lstlisting}

\end{frame}


% 6-3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{Real Vs. Predicted Plots}

\begin{columns}
\begin{column}{0.5\textwidth}
\begin{figure}
\includegraphics[width=0.8\textwidth]{\string"../quantlets/Model_Comparison/Model_Comparison_Presentation\string".pdf}
\end{figure}
\end{column}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item If the house prices were predicted perfectly, all points would lie on the red line
\item The blue line comes from an OLS regression: The better the predictions, the more blue and red lines should align 
\end{itemize}
\end{column}
\end{columns}
}

% 6-4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Performance measure example: MSE Code}

\begin{lstlisting}[language=R]
model.mse = function(model, test.data = test) {
    if (class(model)[1] %in% c("train", "lm")) {
        pred = predict(model, newdata = test.data)
        mse  = (1/ncol(test.data)) * sum((pred - test.data$logSalePrice)^2)
    } else {
        pred = predict(model, newx = as.matrix(test.data[!names(test.data) %in% "logSalePrice"]), s = "lambda.1se")
        mse  = (1/ncol(test.data)) * sum((pred - test.data$logSalePrice)^2)
    }
    return(mse)
}

\end{lstlisting}

\end{frame}

%6-5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{Result comparing all models}

\input{\string"../quantlets/Model_Comparison/modelcomparison\string".tex}
\begin{itemize}
\item In comparison the self built linear model and the forward algorithm habe the best results amongst all implemented models
\item The random forest and the gradient boosting method show a far better Mean Sqared Error on the training data. This might indicate, that these models are overfitted to the data.
\end{itemize}
}









