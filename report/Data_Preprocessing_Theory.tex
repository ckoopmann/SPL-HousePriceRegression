\subsection{Data Preprocessing}\label{sec:data_theory}

Different models are differently influenced by certain characteristics of the data, especially missing values, outliers and dimensionality.  The aim of the quantlet "Data Preprocessing" ist to ensure, that all models that are later used are capable of handling the same training dataset, thus ensuring the comparability of results. 
To begin with, i.e. regression models are not capable of handling missing data which results in a rowwise deletion of cases and therefore a lower number of observations. A possible approach to avoid this is  imputation, for instance with a simple but robust median and mode imputation. For this, missing observations for a variable are replaced with the median or mode of the avaiable observations.

Especially in linear regression models outliers in the data can have a large impact on the regression coefficients (high leverage), and therefore impede the out of sample capabilities of the  models. While a large value might not be necessarily an error, especially in the machine learning context such leverage can be an issue. A possibility to detect and truncate outliers is based on the inter quantile range ($IQR = 3rd Quantile -1st Quantile$).
Often an extreme outlier is defined as a value, that is larger than $3 \times IQR$. We decide to only truncate values that are larger than this treshold. 

Given the possibility of correlated regressors and the large number of predictors, Principal Component Analysis (PCA) can be used to reduce the number of numeric regressors and avoid multicollinearity. This is especially appealing since we are not interested in substantial interpretation of variables, which means that it is no shortcoming that the extracted components in the regression might be more difficult to interpret than the original variables. 
The aim of Principial Component analysis in this context is to transform a number of correlated variables to a lower number of uncorrelated components, while explaining as much of the initial variance as possible. 
Let X be the the data matrix, then the full decomposition is given by 
  $  T = XW $
where W is the matrix that contains the eigenvectors of $X'X$.  
The eigenvectors can be retained using spectral decomposition of the correlation matrix of selected variables.
Instead of keeping all uncorrelated factors, we rely on a subset of factors $T^* \subset T$, such that the number of components is smaller than the number of variables. 
For the categorical variables dimensionality is a result of the number of factor levels. If a factor level only contains a very small number of observations, it might perfectly predict the outcome (see also section \nameref{sec:reg_theory}). Also, because the dataset is later split in different training and tests set, an issue might be that a small factor level does not exist in the training data, but in the test data. To avoid this, small factor levels can be merged to a sufficiently large factor level "other". 