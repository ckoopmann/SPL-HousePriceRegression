\subsection{Evaluation}
In this section we explain, how we implement ways to evaluate our results i.e. compare the prediction models. As stated in the theory part about model evaluation, there are many measures that can be looked at in order to decide which models perform better than others. Since we take a variety of these measures into account we will present examplary code for one of them. All the functions are created the same way and it is therefore sufficient to explain the code for the mean squared error (MSE) function as a blueprint for the others. \\
For the quantlet "Model Comparison" all the trained models, training data called train and test data called test are needed. The function \textit{model.mse} takes two input parameters. The first is the model the mse is supposed to be computed for and the second the test data on which the mse is executed (defaults to test). 
\begin{lstlisting}[language=R]
model.mse = function(model, test.data = test) {
    if (class(model)[1] %in% c("train", "lm")) {
        pred = predict(model, newdata = test.data)
        mse  = (1/nrow(test.data)) * sum((pred - test.data$logSalePrice)^2)
    } else {
        pred = predict(model, newx = as.matrix(test.data[!names(test.data) %in% "logSalePrice"]), s = "lambda.1se")
        mse  = (1/nrow(test.data)) * sum((pred - test.data$logSalePrice)^2)
    }
    return(mse)
}
\end{lstlisting}
Within the function predictions for the \textit{logSalePrice} in the test dataset have to be made first. The models we use differ in prediction functions available. For the lasso and ridge models one has to use the prediction function from the \textit{glmnet} package for example and for a standard linear regression one uses the standard \textit{predict} function from the base \textsf{R} distribution. Using an if-else statement based on the model class the MSE function decides between the variants. After predicting the target variable the MSE is computed and returned using the real data and the model predictions. The other functions we implement (MSE on training data, mean absolute error, bias and $R^2$) differ only in the last step, the computation of the measure itself. All of them use predictions made on the test data based on their respective model class. \\
After the introduction of the evaluation functions we use them to create a table summarizing all the results.
\begin{lstlisting}[language=R]
model.list                  = list(lm.fit, fwd.fit, lasso.fit, ridge.fit, gbmtuned, rftuned)
comparisonMSE.list          = sapply(model.list, FUN = model.mse)
comparisonMSEtrain.list     = sapply(model.list, FUN = model_overfit.mse)
comparisonMAE.list          = sapply(model.list, FUN = model.mae)
comparisonBIAS.list         = sapply(model.list, FUN = model.bias)
comparisonRSQ.list          = sapply(model.list, FUN = model.Rsq)
comparison.result           = matrix(c(round(comparisonMSE.list, 3), round(comparisonMSEtrain.list, 3), round(comparisonMAE.list, 3), 
    round(comparisonBIAS.list, 3), round(comparisonRSQ.list, 3)), ncol = length(model.list), byrow = TRUE)
rownames(comparison.result) = c("MSE", "MSEtrain", "MAE", "BIAS", "RSQ")
colnames(comparison.result) = c("bwd", "fwd", "lasso", "ridge", "gbm", "rf")
\end{lstlisting}
First a list of all trained models is set up. This list is used via the sapply command to create lists of the respective measures for every model. The lists are combined in a matrix of results, the columns being the models and the rows the different measures of prediction accuracy. The resulting table is afterwards saved as a .tex file using the package \textit{xtable}. Additionally to the numeric representation we also show the results graphically. Using \textit{ggplot2} we create predictions vs. real values plots for all models and enhance the graphs by drawing a line, where the points would have to lie, if every datapoint in the test set was predicted correctly. Furthermore we run linear regression of the predicted on the real values and add the resulting regression line to the plots. 
\begin{lstlisting}[language=R]
predicted.values           = data.frame(cbind(predictions.bwd,predictions.fwd,predictions.lasso,predictions.ridge,predictions.rf,predictions.gbm))
colnames(predicted.values) = c("predictions.bwd","predictions.fwd","predictions.lasso","predictions.ridge","predictions.rf","predictions.gbm")
coeff.lm                   = vector("list", ncol(predicted.values)) # preparing an empty list for coefficients of regression

# Looping through all the predictions of the different models
for (i in 1:ncol(predicted.values)){
  data.temp = data.frame(cbind(predicted.values[,i],test$logSalePrice))
  lm.temp = lm(data.temp[,1]~data.temp[,2])
  coeff.lm[[i]] = coef(lm.temp)
}
\end{lstlisting}
This code implements the linear regression needed for the additional line in the scatterplots. All predicted values are combined into a dataframe and an empty list for the regression coefficients is created. We run through the predictions list with a for-loop, creating first temporary dataframes with the predictions and the real values and then running the linear regression. The coefficients of these temporary regressions (intercept and slope) are saved in the list \textit{coeff.lm}. As before with the MSE we only show the code for one plot, since  the codes for the other models are basically identical except the predicted values used.
\begin{lstlisting}[language=R]
df.bwd.fit      = data.frame(cbind(test$logSalePrice, predictions.bwd ))  # creating dataframe containing real and predicted outcome

bwd.plot = ggplot(df.bwd.fit, aes(V1, predictions.bwd)) + geom_point() + geom_segment(x = -4, 
    y = -4, xend = 4, yend = 4, color = "red", size = 1.3) + stat_smooth(method = "lm", se = FALSE) + labs(title = "Backward selection linear model", 
    x = "logSalePrice", y = "bwd.fit predictions") + annotate("text", label = paste("int:", round(coeff.lm[[1]][1],4), sep = " "), x = -3, y = 3, size =10, color = "blue") + annotate("text", 
    label = paste("slope:", round(coeff.lm[[1]][2],4), sep = " "), x = -3, y = 2.5, size =10, color = "blue") + theme_classic(base_size = 20) 
\end{lstlisting} 
In this code snippet we create a dataframe \textit{df.bwd.fit} of real values and predictions from the backwards selection linear model. The object \textit{bwd.plot} is a scatterplot of the variables in the mentioned dataframe. Additionally with the command \textit{geomsegment} a red line is drawn through the point cloud indicating perfect prediction accuracy. \textit{Statsmooth} is used to also include the fittted line of an ordinary least squares regression. Making use of the regressions run outside the plot we include the intercept and slope coefficients via the \textit{annotate} command.   