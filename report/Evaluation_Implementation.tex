\subsection{Evaluation}\label{sec:Evaluation_Implementation}
In this section we explain, how we compared the predictions of our different models and evaluated their relative performance. As stated in Section \ref{sec:Evaluation_Theory}, there are many measures that can be used to decide which models perform better than others. Since we take a variety of these measures into account we will present exemplary code for only one of them. All the functions are created the same way and it is therefore sufficient to explain the code for the Mean Squared Error (MSE) function as an illustration of our approach.

For the quantlet \textit{Model Comparison} all the trained models, as well as the training and test data are needed as input. The function \textit{model.mse} takes two input parameters. The first is the model the MSE is supposed to be computed for and the second is the test data on which the MSE is to be evaluated (defaults to test). 
\begin{lstlisting}[language=R]
model.mse = function(model, test.data = test) {
    if (class(model)[1] %in% c("train", "lm")) {
        pred = predict(model, newdata = test.data)
        mse  = (1/nrow(test.data)) * sum((pred - test.data$logSalePrice)^2)
    } else {
        pred = predict(model, newx = as.matrix(test.data[!names(test.data) %in% "logSalePrice"]), s = "lambda.1se")
        mse  = (1/nrow(test.data)) * sum((pred - test.data$logSalePrice)^2)
    }
    return(mse)
}
\end{lstlisting}
Within the function predictions for the \textit{logSalePrice} in the test dataset have to be made first. The models we use differ regarding the available prediction functions. For the lasso and ridge models one has to use the prediction function from the \textit{glmnet} package for example and for a standard linear regression one uses the standard \textit{predict} function from the \textit{base} package. Using an if-else statement based on the model class the MSE function selects the appropriate function. After predicting the target variable the MSE is computed and returned using the actual values of \textit{logSalePrice} and the model predictions. The other functions we implement (MSE on training data, Mean Absolute Error, bias and $R^2$) differ only in the last step, which is the computation of the measure itself. All of them use predictions made on the test data based on their respective model class. 

After the introduction of the evaluation functions we use them to create a table summarizing all the results.
\begin{lstlisting}[language=R]
model.list                  = list(lm.fit, fwd.fit, lasso.fit, ridge.fit, gbmtuned, rftuned)
comparisonMSE.list          = sapply(model.list, FUN = model.mse)
comparisonMSEtrain.list     = sapply(model.list, FUN = model_overfit.mse)
comparisonMAE.list          = sapply(model.list, FUN = model.mae)
comparisonBIAS.list         = sapply(model.list, FUN = model.bias)
comparisonRSQ.list          = sapply(model.list, FUN = model.Rsq)
comparison.result           = matrix(c(round(comparisonMSE.list, 3), round(comparisonMSEtrain.list, 3), round(comparisonMAE.list, 3), 
    round(comparisonBIAS.list, 3), round(comparisonRSQ.list, 3)), ncol = length(model.list), byrow = TRUE)
rownames(comparison.result) = c("MSE", "MSEtrain", "MAE", "BIAS", "RSQ")
colnames(comparison.result) = c("bwd", "fwd", "lasso", "ridge", "gbm", "rf")
\end{lstlisting}
First a list of all trained models is set up. This list is used via the \textit{sapply} function to create lists of the respective measures for every model. The lists are combined in a matrix of results, the columns being the models and the rows the different measures of prediction accuracy. The resulting table is afterwards saved as a \textit{.tex} file using the package \textit{xtable}. Additionally to the numeric representation we also show the results graphically. Using \textit{ggplot2} we plot actual values against   for all models predictions and enhance the graphs by drawing a line, where the points would have to lie, if every data point in the test set was predicted correctly. Furthermore we run a linear regression of the actual values on the predicted and add the resulting regression line to the plots. 
\begin{lstlisting}[language=R]
predicted.values           = data.frame(cbind(predictions.bwd,predictions.fwd,predictions.lasso,predictions.ridge,predictions.gbm,predictions.rf))
colnames(predicted.values) = c("predictions.bwd","predictions.fwd","predictions.lasso","predictions.ridge","predictions.rf","predictions.gbm")
coeff.lm                   = vector("list", ncol(predicted.values)) # preparing an empty list for coefficients of regression

# Looping through all the predictions of the different models
for (i in 1:ncol(predicted.values)){
    data.temp = data.frame(cbind(test$logSalePrice,predicted.values[,i]))
    lm.temp = lm(data.temp[,1]~data.temp[,2])
    coeff.lm[[i]] = coef(lm.temp)
}
\end{lstlisting}
This code implements the linear regression needed for the additional line in the scatterplots. All predicted values are combined into a dataframe and an empty list for the regression coefficients is created. We run through the predictions list with a for-loop, first creating temporary dataframes with the predictions and the real values and then running the linear regression on this data. The coefficients of these  regressions (intercept and slope) are saved in the list \textit{coeff.lm}. As before with the MSE we only show the code for one plot, since  the codes for the other models follows the same approach.
\begin{lstlisting}[language=R]
df.bwd.fit  = data.frame(cbind(test$logSalePrice, predictions.bwd ))  # creating dataframe containing real and predicted outcome

bwd.plot    = ggplot(df.bwd.fit, aes(predictions.bwd,V1)) + geom_point() + geom_segment(x = -4, y = -4, xend = 4, yend = 4, color = "red", size = 1.3) +
    stat_smooth(method = "lm", se = FALSE) + labs(title = "Backward selection linear model", x = "bwd.fit predictions", y = "logSalePrice") +
    annotate("text", label = paste("int:", round(coeff.lm[[1]][1],4), sep = " "), x = -2, y = 3, size =10, color = "blue") +
    annotate("text", label = paste("slope:", round(coeff.lm[[1]][2],4), sep = " "), x = -2, y = 2.5, size =10, color = "blue") + 
    theme_classic(base_size = 20) 
\end{lstlisting} 
In this code snippet we create a dataframe \textit{df.bwd.fit} of real values and predictions from the backwards selection linear model. The object \textit{bwd.plot} is a scatterplot of the variables in the mentioned dataframe. Additionally with the command \textit{geomsegment} a red line is drawn through the point cloud indicating perfect prediction accuracy. \textit{Statsmooth} is used to also include the fittted line of an ordinary least squares regression. Making use of the regressions run outside the plot we include the intercept and slope coefficients via the \textit{annotate} command.   