
\subsubsection{Regression Models}
The self implemented backwards selection converges after three iterations. The final iteration returns 32 variables which are all significant at a level of at least $\alpha=0.05$. Together these variables have an 
The $R^{2}$ is about 0.90, which means that about 90\% of variance is explained by this model (in the training data!).
The forward selection based on the package \textit{step} only shows a slightly higher explanatory power, but selects about twice as much variables.  The stepwise regression converges at 77 variables, with an $R^{2}$ of about 0.91. 
\autoref{fig:step} visualizes the decrease in the $AIC$ depending on the number of variables that are included. The decrease in $AIC$ is very large in the beginning, and smaller towards the end. 


For the Lasso model, \autoref{fig:Lasso} shows the crossvalidation results. Shown here is  $MSE$ versus the logarithm of the different $\lambda$ values that were tried in the CV-procedure. The dashed vertical lines show the $\lambda$ where the $MSE$ is minimal (left line) respectively the first standard deviation of that $\lambda$ value (right line). Thus, the model suggested by the cross validation uses a $\lambda = \exp{-2.85}=0.058$. 

Based on this hyperparameter 33 variables have a coefficient unequal $0$, resulting in a $R^{2}$ of 0.87. 
The trace plot (\autoref{fig:Lasso1}) visualizes how the regularization in Lasso results in both sparsity and shrinkage. The graph shows how the coefficients change depending on the $L^{1}$-norm, which is the sum of regularized coefficients. If there is no regularization, which is the case when the $L^{1}$ norm is allowed to be (infinitely) large,  the solution equals the OLS solution with all variables. For a small $L^{1}$-norm there is a lot regularization and the model gets sparser, which is a result of coefficients that are shrunk to zero. The trace plot for instance shows, that the variable  "Comp. 1", which is the first extracted component (see section \nameref{sec:data_theory}) is the most important variable, as it is the variable that is even in the most regularized model.

For Ridge, the hyperparameter is $\lambda = 1.06$, which results in in the same $R^{2}$ value. As expected (see section \nameref{sec:reg_theory}), Ridge returns the original number of variables. 

\autoref{table:regs} summarizes the results of the different selection mechanisms. Apart from the above described differences in number of variables and  $R^{2}$ the table shows, that the regularized models also include variables that are not significant in the stepwise models, which means that at least in the training sample these variables appear to have some predictive power. Overall , the predictive performance is similar, but there are clear differences in the number of variables, and which variables are considered important. 
\begin{figure}[H]
  \centering
\includegraphics[width=0.7\textwidth,keepaspectratio]{\string"../quantlets/Regression_Models/step\string".pdf}
  \caption{AIC versus number of variables in forward stepwise regression}\label{fig:step}
\end{figure}
 
\begin{figure}[H]
  \centering
\includegraphics[width=0.7\textwidth,keepaspectratio]{\string"../quantlets/Regression_Models/Lasso_lambda\string".pdf}
  \caption{Mean squared error versus the tested lambda values. Results from cross validation procedure for Lasso.}\label{fig:Lasso}
\end{figure}

\begin{figure}[H]
  \centering
\includegraphics[width=0.8\textwidth,keepaspectratio]{\string"../quantlets/Regression_Models/Lasso\string".pdf}
  \caption{Illustration of variable selection for Lasso based on the regularization term}\label{fig:Lasso1}
\end{figure}

 
\FloatBarrier