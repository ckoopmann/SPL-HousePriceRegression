\subsection{Regression Models: Theory}
As it is well known from the Gauss-Markov theorem, there is no better linear unbiased estimator than the OLS estimator $\beta_{ols} = (X'X)^{-1}X'Y$  if the conditions for GM are met. However, in the here considered scencario we have a large number of possible regressors given the number of observations. Thus, including all variables in an OLS model would likely lead to overfitting, where our OLS-model would fit well on the training data, but would not generalize well to other data, especially the data it is later supposed to be applied on. In the worst case, perfect multicolinearity could occur (especially with regard to the dummy variables)  which means that some regressors are linear comibinations of the other. In this case, the inverse of the matrix $(X'X)$ does not exist any more beacuse of singularity which means $\beta_{OLS}$ cannot be calculated.  
On the other hand, we still want to include all relevant variables, as we might oitherwise encounter omitted variable bias, and would also reduce the predicitve capabilities of our model. 
While in econometric models  the selection of variables is usually based on theory, in the sceneario of this Kaggle challenge we do not have theoretical assumptions about our dataset. Therefore an alternative procedure for variable selection is needed. This quantlet implements four regression based methods and tests them on the Amis housing data. 

\subsubsection{Backward Selection}
One straightforward approach is backwards stepwise regression. While there are several approaches, one possibility is to  start with all variables (except those that are perfect linear combinations of others), and than eliminate insignificant variables based on a series of  t-tests. This procedure is repeated, until every variable is significant. This requires a decision on the threshold on which the significance of a regressor is decided, which is an arbritary number. Here, we rely on a $\alpha = 0.05$ threshold, as it is commonly used in econometric applications. 

\subsubsection{Forward Selection}
The second implemented model is forward selection based on the AIC (Akaike Information Criterion). The AIC is defined as 
${\displaystyle \mathrm{AIC}=2k-2\ln({\hat{L}})}$, where $\hat{L}$ is the likelihood of a given model. The stepwise algorithm keeps adding variables until there is no further decrease in the AIC possible. This situation occurs, when the penalty term $2k$ exceeds the difference in likelihood due to the inclusion of further variables. 

\subsubsection{Ridge and Lasso Regression}
Ridge and Lasso regressions omit the assumption of an unbiased estimator. Both models include a penalizing term for the regressors, and therefore minimize the penalized sum of squares.  Written in a Lagrange - Multiplier form it is easy to see, that the only difference between both estimators is the $L^{p}$-norm: Ridge relies on an $L^{2}$-norm, while Lasso uses an  $L^{1}$-norm.  

\begin{equation}
\beta_{lasso}=argmin\{\frac{1}{2}.\sum_{t=1}^{N}(y_{t}-\beta_{0}-\sum_{j=1}^{p}x_{ij}\beta_{j})^{2}+\lambda\sum_{j=1}^{p}|\beta_{j}|\}
\end{equation}

\begin{equation}
\beta_{ridge}=argmin\{\frac{1}{2}.\sum_{t=1}^{N}(y_{t}-\beta_{0}-\sum_{j=1}^{p}x_{ij}\beta_{j})^{2}+\lambda\sum_{j=1}^{p}\beta_{j}^{2}\}
\end{equation}

The different $L^{p}$ norm however leads to a different behavior in both models: Ridge uses L2 penalty term which limits the size of the coefficient vector, a behaviour that is referred to as shrinkage. Thus, Ridge will return the original number of variables.  Lasso on the other hand will also result in sparsity, which means that some coefficients will get the value zero and therefore drop out of the model. 

The question is now, how the hyperparamater $\lambda$ is chosen. We follow a cross validation approach, as it is common practice in machine learning. For this, a sequence of possible $\lambda$ values is tested on different splits into training and test data of the initial sample.  We rely on  k-fold cross-validation, where the dataset is split in k folds, and then each split is used once as testing data for validadtion, while the remaining folds funciton as training data. 
Based on the Mean Squared Error, the optimal $\lambda$ parameter is then chosen. We use the first Standard deviation of the $\lambda$, that yields the lowest MSE, as this leads to more robust results, since overfitting is avoided. 

it should be highlighted, that there is some critizism in the literature regarding the use of both stepwise as well as regularized models, especially that there use should be avoided if there are theoretical assumptions about the data and the interest lies in statistical inference. Since this is not the case and we are only concerned about the predictive performance of our models, this criticism can be relaxed. 



