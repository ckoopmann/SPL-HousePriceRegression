\subsection{Random Forest}
A Random Forest Regression model is an ensemble of Regression Trees. Therefore in this section we first explain the way in which a single Regression Tree works and afterwards we illustrate the method of creating an ensemble method and explain how this applies to the Random Forest.
\subsubsection{Regression Trees}
Just as its close relative the Classification Tree the Regression Tree is based on the principle of recursive transition (\cite{liaw_classification_2002}). This method divides the dataset into separate subsets based on a series of binary tests. In the graphical representation of a tree each of these tests is represented by a node. Based on whether the test returns \textit{True} or \textit{False} for a given observation this observation follows down one of the two edges going out of the node until it ends in one of the so called \textit{Leafnodes} at the bottom of the tree. The predicted value of the target variable then corresponds to the average value within that Leafnode. With $L_i$ the index set of all observations in the same Leafnode as observation $i$ and $n_i$ the number of such observations the predicted value for Observation $i$ $\tilde{y}_i$ is:
\begin{align}
\tilde{y}_i = \frac{1}{n_i}\sum_{j \in L_i} y_j
\end{align}
In Figure  \ref{fig:tree} we have plotted an example of a single regression tree trained on the logarithm of the Sale Price of the Housing Data. For each node the mean logarithmic Sale Price as well as the proportion of cases in that node are given. 
\begin{figure}
  \centering
\includegraphics[width=0.8\textwidth,keepaspectratio]{\string"../quantlets/SPL_Random_Forest/tree\string".png}
  \caption{Example Regression Tree predicting log SalePrice}\label{fig:tree}
\end{figure}

One advantage of this method is its ability to very easily incorporate both numeric as well as non-numeric variables into the model. This is possible since we can define binary tests on all kinds of variables. Another advantage is the computational efficiency when predicting the value for a new observations. However in our context maybe the most interesting feature of this method is the fact that, with sufficient depth of the tree it can approximate arbitrarily complex functions and is in no way limited to linear relationships. While the prediction of new cases using a regression tree is relatively straightforward it is not immediately clear how one might go about finding a good tree, since the loss function is not as easily differentiable as that of a linear model. A regression tree is usually iteratively grown using the following procedure.
\begin{enumerate}
\item Start with all data as a parent node
\item Do a search over binary splits evaluating the objective function $S$ at each split
\item Choose split that minimizes $S$
\item Check if stopping criteria is met
\begin{enumerate}
	\item If stopping criteria is met Stop
	\item If stopping criteria is not met, repeat previous steps using each child node as parent node.
\end{enumerate}
\end{enumerate}
The remaining challenge is to define suitable objective functions and stopping criteria. In the case of the regression tree one usually chooses the sum of squared errors across all nodes as objective function. With $n_l$ the number of all leaves at the current split and $L_i$ the index set of the i-th leaf and $m_i$ the mean at that leaf, we can write this as:
\begin{align}
S = \sum_{i = 1}^{n_l} \sum_{j \in L_i} (y_j - m_i)^2 \label{eq:obj1}
\end{align}
With $n_i$ the number of observations in the i-th leaf and $V_i$ the variance within that leaf we can rewrite equation \ref{eq:obj1} as:
\begin{align}
S = \sum_{i = 1}^{n_l}n_i * V_i
\end{align}
Without a stopping criteria, this objective function could always be reduced by an additional split for any node that contains more than one unique combination of input and output parameters. That would lead to a very deep tree where each leaf node contains only one observation and would therefore be likely to overfit the training data. Such stopping criteria can be for example a maximum tree depth or a minimum reduction of $S$ for an additional split. Another method to avoid model complexity and over-fitting is to first grow the full tree (i.e. with one observation per leaf node) and then prune (i.e. merge some nodes back together) based on some criteria.
\subsubsection{Random Forest as Ensemble of Trees}
The basic idea of an ensemble is to combine the predictions of several independently trained models to get more accurate predictions. In the case of a continuous target variable one usually takes a weighted sum of the individual model predictions as the new prediction. With $n$ models and $\tilde{y}_i^j$ the prediction of the j-th model for the i-th observations and $w_j$ the weight of that model we get:
\begin{align}
\tilde{y}_i = \sum_{j = 1}^n w_j * \tilde{y}_i^j
\end{align}
The simplest weighting method would be to weigh all models equally and set $w_j = \frac{1}{n}$ for all $j$. In 2001 Breiman combined the ideas of model ensembles and decision trees to develop the Random Forest (\cite{breiman_random_2001}). The algorithm works in the following way given the parameters $n_{trees}$, $n_{sample}$, $n_{variables}$:
\begin{enumerate}
\item Draw a bootstrap sample of size $n_{sample}$ from the data
\item Draw a random sample of size $n_{variables}$ from all input variables without replacement
\item Grow a full decision tree on the sampled observations and variables
\item Repeat step 1-3 $n_{trees}$ times to get as many trees
\item Combine all trees in an ensemble (i.e. average predictions in regression case)
\end{enumerate}
Random forests have shown to be successful at a variety of different classification and regression tasks. They are very popular thanks to the lack of assumptions made on the type of variables or their distribution as well as the relative simplicity of the underlying idea. While training of a large forest can be computationally expensive, generating new predictions is very quick and does not take a lot of resources.
\subsubsection{Variable Importance in Random Forests}
While Random Forests are versatile models with often surprisingly accurate predictions, one of their main drawbacks is the fact that they belong to the class of so called "black box" models. This means that unlike linear models we can not easily read of the effect one specific variable has on the target since we have no $\beta$ parameters.
To still be able to asses the importance of the different variables a variety of methods. One importance measure that is often used and will also be included in the later sections of this report is the \textit{Scaled Permutation Importance} (\cite{liaw_classification_2002}). To calculate this measures one takes the following steps:
\begin{enumerate}
\item Train Random Forest
\item For one variable randomly permutate all values of that variable
\item Compute difference in prediction accuracy (Sum of Squared Errors for Regression) before and after permutation
\item Repeat step 2-3 for all variables
\item Divide each difference in accuracy by the maximum difference across variables to get values between 0  and 1
\end{enumerate}
The idea behind this measure is that changing a highly important variable should have a larger effect on the accuracy than that of an unimportant variable.