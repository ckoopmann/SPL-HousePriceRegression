\subsection{Evaluation}
After tuning and training all our models it is important to test their performance on "unseen" data, which is not included in the training of the models. For this purpose we hold back our test dataset on which we evaluate our models with different metrics namely the mean squared error (MSE), the mean squared error on the training data (looking for possible overfitting), the mean absolute error (MAE), the bias and the $R^2$. As one can see in Table \ref{tab:measures} the results differ substantially from model to model sometimes. Looking at the MSE first, we identify the self built backwards selection linear model, the forward selection model and the gbm model as best with regards to this metric. Lasso and Ridge regression have the worst values. This picture is confirmed by the other measures implemented. The linear models without regularization components and the gbm always show the best results, whereas the random forest gives "middle of the road" numbers and Lasso and Ridge regression have the worst results. One exception is the bias, where gbm has the highest figure, but in that case all models come off very similar. One interesting thing, that can be taken away from Table \ref{tab:measures} as well, is the concern of possible overfitting for the gbm and random forest model. Especially the random forest has a really low MSE value on the training dataset (lower than on the test dataset, which is unique amongst all models). 

\input{\string"../quantlets/Model_Comparison/modelcomparison\string".tex}

Besides the numbers presented in Table \ref{tab:measures} we give a graphical representation of our results. Every model is depicted in a scatterplot of the real \textit{logSalePrice} in the test dataset on the ordinate and the predicted values on the abscissa. Furthermore two lines are drawn. The red line indicates perfect prediction accuracy as mentioned in the implementation section about model evaluation. If all the points of the cloud lied on this line every price would be predicted with the correct value. That is of course unrealistic in practice, wherefore the points should be grouped as closely as possible around the line as possible to have a good prediction result. The second blue line stems from a linear regression of the predictions on the real data. The reasoning behind this, is that the intercept and slope of the regression should be the same as of the red line (i.e. intercept = 0, slope = 1) if the model predicts well. A generell observation can be taken away from the graphs. All models seem to overestimate in the lower ranges of \textit{logSalePrice} (points mostly above the red line) and underestimate in the higher ranges (points mostly below the red line). This is confirmed by the slopes of the blue lines, since all of them are smaller than the ideal slope of one. Overall the graphs support, what can be seen in Table \ref{tab:measures}. For example looking at Figure \ref{fig:lasso} and Figure \ref{fig:ridge}, the point clouds are less tightly grouped around the red line, than in the other scatterplots. They also have the regression slopes (Lasso: 0.7905, Ridge: 0.7825) that differ the most from one. For the models with better results in the table, we see the opposite. Despite also not having a regression slope of one they come much closer (e.g. forward selection model: 0.9175 in Figure \ref{fig:fwd}). With the analysis both numerical and graphical the linear models without regularization perform very well in contrast to Lasso and Ridge regression models. Gbm also predicts well, but has also a lower value for the MSE on the training data. This gives rise for a little concern of overfitting.    




\begin{figure}[H]
\centering
	\includegraphics[width=0.7\textwidth,keepaspectratio]{\string"../quantlets/Model_Comparison/lm_fit\string".pdf}
  	\caption{Scatterplot of linear model predictions and test data}
  	\label{fig:lm}
\end{figure}

\begin{figure}[H]
\centering
	\includegraphics[width=0.7\linewidth,keepaspectratio]{\string"../quantlets/Model_Comparison/fwd_fit\string".pdf}
  	\caption{Scatterplot of forward model predictions and test data}
  	\label{fig:fwd}
\end{figure}

\begin{figure}[H]
\centering
	\includegraphics[width=0.7\linewidth,keepaspectratio]{\string"../quantlets/Model_Comparison/lasso_fit\string".pdf}
  	\caption{Scatterplot of lasso model predictions and test data}
  	\label{fig:lasso}
\end{figure}

\begin{figure}[H]
\centering
	\includegraphics[width=0.7\linewidth,keepaspectratio]{\string"../quantlets/Model_Comparison/ridge_fit\string".pdf}
  	\caption{Scatterplot of ridge model predictions and test data}
  	\label{fig:ridge}
\end{figure}

\begin{figure}[H]
\centering
	\includegraphics[width=0.7\linewidth,keepaspectratio]{\string"../quantlets/Model_Comparison/gbmtuned\string".pdf}
  	\caption{Scatterplot of gbm model predictions and test data}
  	\label{fig:gbm}
\end{figure}

\begin{figure}[H]
\centering
	\includegraphics[width=0.7\linewidth,keepaspectratio]{\string"../quantlets/Model_Comparison/rftuned\string".pdf}
  	\caption{Scatterplot of random forest model predictions and test data}
  	\label{fig:rf}
\end{figure}

